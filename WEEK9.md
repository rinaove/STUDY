# 6장: 통계적 머신러닝 
## 6-1. K-최근접 이웃
* 특징: 회귀와 달리 모델 피팅하는 과정 필요X (학습과정X)
* 특징2: 예측변수들은 수치형이어야 함
* 희귀 데이터를 분류할 때는
    * 단순히, 이진형 결과가 아니라 0과 1사이의 확률값을 갖게 설정할 수 있음

> 유클리드 거리 vs 맨해튼 거리 
📌 언제 유리할까?
* 유클리드 거리: 연속형 수치, 각 변수의 단위가 같을 때 적합.
    * 주로 이미지 픽셀 퍼리, PCA, K-means 클러스터링에서 사용
* 맨해튼 거리: 변수의 분포가 비정규거나, 이상치가 많거나, 차원이 높은 경우 더 견고.
    * 텍스트 데이터, 이상치가 많은 금융 데이터 사용
    ```
    문서1: "I love cats" → [1, 1, 1, 0]
    문서2: "I love dogs" → [1, 1, 0, 1]
    단어 순서: I, love, cats, dogs
    대부분 0이 많고 희소함
    ```

> 원-핫 인코더
* 하나의 비트만 양수가 허용되는 회로 설정 
* 보통, 선형회귀나 로지스틱 회귀에서 다중공선성 문제로 한 가변수를 생략함 

> KNN
* 최적의 k 찾기 1) 교차 검증(Cross-Validation)

```Ruby
여러 개의 k 후보값에 대해 모델을 학습하고,
교차 검증 오차(예: MSE, Accuracy)를 비교하여
가장 성능이 좋은 k를 선택

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

k_list = range(1, 31)
scores = []

for k in k_list:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')
    scores.append(score.mean())

best_k = k_list[np.argmax(scores)]
```
* 최적의 k 찾기 2) 학습곡선 + 편향-분산 트레이드오프 고려
    * k=1, 분산이 커짐, 과적합
    * K=30, 편향 커짐, 과소적합 (과대 평탄화는 과소적합의 한 형태로 볼 수 있음)

## 6-2. 트리 모델
* 트리 모델: 분류 분석
    * 랜덤 포레스트 및 부스팅 트리: 데이터 과학에서 사용되는 예측 모델링 기법 기초 
* 용어
    * 재귀 분할: 데이터를 반복적으로 쪼개면서 분류하거나 예측하는 트리 기반 기법의 핵심 원리. 동질성 높은 노드로 나누는 과정
    * 손실: 분류 과정에서 발생하는 오분류의 수. 
    * 불순도: 데이터 분할 집합에서 서로 다른 클래스의 데이터가 섞여있는지 (=이질성)
    * 가지치기: 학습 끝난 트리 모델에서 오버피팅 줄이기 위해 가지치기 하는 과정
```Ruby
loan_tree = DecisionTreeClassifier(
    random_state=1,
    criterion='entropy',
    min_impurity_decrease=0.003
)

설명: 결정 트리 분류 모델을 생성합니다.
random_state=1: 결과 재현 가능하게 시드 고정 
criterion='entropy': 분할 기준으로 정보이득 (Information Gain) 사용 (얼마나 잎이 동질해지는가)
min_impurity_decrease=0.003: 분할할 때 정보 불순도(entropy)가 0.003 이상 줄어들지 않으면 더 이상 분할하지 않음 → 과적합 방지용 하이퍼파라미터
```
> 재귀 분할 알고리즘
* 나눠진 그룹 내 동질성 측정
* 여러 분할값 중 동질성이 가장 높은 분할값 찾음 (지니 불순도와 엔트로피라는 불순도 측정 지표. 보통 범주형 변수에서 사용 많이 함)
* 즉, 모든 Xj 중에서 동질성이 가장 높아지게 나누는 Xj와 sj 조합을 고르기

> 트리 형성 중지
* 최소 분할 영역 크기나 말단 잎 크기 조절
* 복잡도 파라미터 (Cp) 설정
    * 교차타당성 검증을 통해 최적의 cp 추정 (최소의 에러를 보이는 cp를 타당성검사 때마다 기록)

> 단일 트리 vs 다중 트리 (6.3과 연결)
| 항목       | 단일 트리 (Single Tree)                                | 다중 트리 (Multiple Trees)                                      |
|------------|---------------------------------------------------------|------------------------------------------------------------------|
| 정의       | 하나의 결정 트리만 학습하여 예측에 사용                | 여러 개의 트리를 조합하여 예측                                  |
| 대표 알고리즘 | DecisionTreeClassifier, DecisionTreeRegressor          | Random Forest, XGBoost, AdaBoost, Gradient Boosting 등          |
| 예측 방식  | 하나의 트리 결과만으로 예측                            | 여러 트리의 결과를 평균 또는 다수결 투표로 예측                 |
| 장점       | 빠르고 해석 용이, 시각화 가능                          | 정확도 높고 일반화 성능 우수, 과적합에 강함                     |
| 단점       | 과적합에 취약, 데이터 변화에 민감                       | 느리고 복잡함, 해석 어려움                                      |
| 적합한 상황 | 설명력 중요, 단순 문제, 피처 적은 경우                | 예측 정확도 최우선, 피처 많고 문제 복잡한 경우                  |


## 6.3 배깅과 랜덤 포레스트
* 앙상블 모델: 여러 모델의 집합 (모델 평균화)
    * 전체 분산 감소 -> 성능 햐앙
    * 같은 데이터에 대해 여러 모델을 만들고 결과 기록 -> 각 레코드에 대해 예측된 결과 평균 (가중평균, 다수결 투표)

> 배깅
* 부투스트랩 종합의 줄임말 
* 매번 부트스트랩 재표본에 대한 새로운 모델 만듬 (다른 모델과의 차이)
* 배깅 추정치
    * 회귀 -> 평균값
    * 분류 -> 다수결 투표

> 랜덤 포레스트
* 배깅 + 피쳐 무작위 선택(레코드 표본 추출 시 변수 역시 샘플링함)
    * 랜덤 피처 선택: 각 노드 분할 시, 전체 피처 중 일부만 무작위로 선택. 이 피처들 중에서 최적 분할 기준을 선택 → 트리 간 다양성 증가 + 상관성 감소
    * 보통, 변수 개수가 P가 일 때 제곱근P개를 선택한다고 함. 

> 주머니 외부(OBB)
* 각 트리를 훈련할 때 사용되지 않은 데이터 샘플
* 추정 에러: 각 훈련 샘플을 제외한 트리들로 그 샘플을 예측하고, 예측이 틀린 비율
* 트리를 추가할 때마다 정확도 개선됨

> 변수 중요도
* R에서는 `importance=True`, 파이썬에서는 `drop_first=True`
* 계산
    * 각 노드에서 분할할 때 감소된 불순도(예: 지니)를 기록
    * 그 분할에 사용된 변수에 대해 누적 기여도를 계산
    * 모든 트리에서 평균낸 값을 정규화하여 0~1 사이 중요도 점수로 표시

> 하이퍼파라미터 In 랜덤포레스트
* nodesize/min_samples_leaf: 말단 노드 사이즈
* maxnodes/max_leaf_nodes: 전체 노드의 최대 개수 

> 부스팅 
* 이상치에 민감 (편향성)
* 연속된 라운드마다 잔차가 큰 레코드들에 가중치 높여 일련의 모델 생성
```
처음엔 모든 샘플에 동일한 가중치
틀린 샘플은 다음 모델에서 가중치를 더 크게 줌
여러 모델의 가중합을 최종 예측에 사용
```
> XG 부스트
* subsample: 각 트리를 학습할 때, 전체 데이터 중 몇 %만 랜덤하게 사용할지 설정
* eta:  각 트리가 최종 예측에 기여하는 비율을 조절하는 계수 (오버 피팅 방지, 업데이트 속도를 줄여 점진적 학습에 기여)
    * eta가 작을수록(점진적 학습) 더 많은 트리 필요 (n_estimator)
* 정규화: 모델 복잡도에 따라 벌점 추가 (alpha: L1정규화 lambda: L2 정규화)

> 하이퍼파라미터와 교차타당성 검사 
* 설정한 하이퍼파라미터 조합마다 폴드에 대한 오차 평균 계산 -> 전체에서 가장 낮은 평균 오차 갖는 최적의 하이퍼파라미어 조합 찾기  


# 7장: 비지도 학습
## 7.1 주성분분석
* 주성분분석은 수치형 변수가 어떤 식으로 공변하는지 알아내는 기법
* 정의: 전체 변수들의 변동성을 거의 대부분 설명할 수 있는 적은 수의 변수들의 집합
    * 가중치가 양수: 그 변수의 값이 커지면, 주성분 점수도 커짐
    * 가중치가 음수: 그 변수의 값이 커지면, 주성분 점수는 작아짐
```Ruby
PC1 = -0.75 × 스트레스 점수 + 0.4 × 수면시간 + 0.3 × 운동량
절댓값이 가장 큰 건 스트레스 점수 (-0.75)

이 경우, PC1은 '건강함' 정도를 반영하는 주성분일 수 있음:
```

> 해석
*  주성분의 최대 개수는 원래 변수 개수와 같거나 작아야 함.

> 대응 분석
* 범주형 피쳐 간 연관성 인식 

## 7.2 k-평균 클러스터링
* 클러스터의 평균과 포함된 데이터들의 거리 제곱합이 최소가 되도록 해야 함. 
* k평균은 k개의 모든 클러스터의 내부 제곱합이 최소가 되도록 래코두둘을 클러스터에 할당
* 클러스터 해석과 평균 중요

> K-평균 알고리즘 
```Ruby
**K개의 중심(centroid)**을 무작위로 설정
각 데이터 포인트를 가장 가까운 중심에 할당
각 군집에 대해 새로운 중심 계산 (평균 좌표)
중심이 변하지 않을 때까지 2~3번 반복
```
* 부호는 방향성보다, 서로 가까운 위치에 있는 레코드들의 그룹 찾는 것이 목적 

> 클러스터 개수 선정
* 팔꿈치 방법 이용

## 7.3 계층적 클러스터링
* 특이점이나 비장상적인 그룹이나 레코드 발견에 특화 
* 덴드로그램에서 트리의 가지 길이는 클러스터 간 차이 정도 

> 병합 알고리즘 
* 정의: 유사한 클러스터들을 반복 병합 
    * 각 데이터 포인트를 개별 클러스터로 시작 → 즉, N개의 데이터면 N개의 클러스터
    * 가장 가까운 두 클러스터를 합침
    * 클러스터 개수가 1개가 될 때까지 반복

> 비유사도 측정 
* 단일연결: 두 클러스터 간 최소 거리
* 완전연결: 두  클러스터 간 최대 거리
* 평균연결: 모든 거리 쌍의 평균 (절충안)
* 최소분산: 워드기법으로 클러스터 내 제곱합 최소화 (K-평균과 유사)
    * Ward 방법과 K-평균은 둘 다 군집 내의 "제곱 오차합(Within-Cluster Sum of Squares, WCSS)"을 최소화하려고 함
![WEEK_9](image/WEEK_9.png)


* 비유사도 측정 방법 
    * 클러스터 A와 B의 모든 레코드 쌍의 최대 거리 계산 (완전연결)

## 7.4 모델 기반 클러스터링
> 다변량정규분포
정의: 여러 변수들이 정규성을 가지면서,서로 상관관계를 가지는 확률 분포

## 7.5 스케일링과 범주형 변수 
* 고워거리: 수치형과 범주형 데이터가 섞인 경우 모든 변수가 0~1사이 오도록 설정 
```
✅ 언제 쓰냐?
군집 분석: 혼합형 변수로 K-평균이 불가능할 때

거리 기반 분류(KNN): 범주형 포함할 때

계층 군집 분석 (Hierarchical Clustering): mixed-type 변수에 적용 가능
```